{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4745dbfe",
   "metadata": {},
   "source": [
    "# CCS-249 Exercise Unit 2: ELIZA and RegEx NLP\n",
    "\n",
    "## Exercise 1: Updating ELIZA\n",
    "Builds a chatbot that recognizes patterns using regex and responds to specific questions. Includes 5 emotional patterns and sarcastic responses for repeated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Updating ELIZA\n",
    "import re\n",
    "import random\n",
    "\n",
    "def reflect(fragment):\n",
    "    \"\"\"Swap pronouns to reflect user input back to them.\"\"\"\n",
    "    reflections = {\n",
    "        \"am\": \"are\",\n",
    "        \"was\": \"were\",\n",
    "        \"i\": \"you\",\n",
    "        \"i'd\": \"you would\",\n",
    "        \"i've\": \"you have\",\n",
    "        \"i'll\": \"you will\",\n",
    "        \"my\": \"your\",\n",
    "        \"are\": \"am\",\n",
    "        \"you've\": \"I have\",\n",
    "        \"you'll\": \"I will\",\n",
    "        \"your\": \"my\",\n",
    "        \"yours\": \"mine\",\n",
    "        \"you\": \"me\",\n",
    "        \"me\": \"you\"\n",
    "    }\n",
    "    return ' '.join(reflections.get(word.lower(), word) for word in fragment.split())\n",
    "\n",
    "def eliza_response(user_input):\n",
    "    \"\"\"Generate ELIZA response based on pattern matching.\"\"\"\n",
    "    patterns = [\n",
    "        (r\"I need (.*)\", \"Why do you need {0}?\"),\n",
    "        (r\"Why don['']t you (.*)\", \"Do you really think I don't {0}?\"),\n",
    "        (r\"I feel (.*)\", \"Tell me more about feeling {0}.\"),\n",
    "        (r\"I want to know the reasons why I am feeling depressed all the time\\.?\",\n",
    "         \"Why do you want to know the reasons for feeling depressed all the time?\"),\n",
    "        (r\"I am feeling stressed\\.?\", \"Tell me more about why you're feeling stressed.\"),\n",
    "        (r\"My feelings towards my crush are invalidated\\.?\",\n",
    "         \"Why do you think your feelings towards your crush are invalidated?\"),\n",
    "        (r\"You (don['']t|do not) understand me\\.?\",\n",
    "         \"Why do you think I don't understand you?\"),\n",
    "        (r\"I (can['']t|cannot) focus on my studies\\.?\",\n",
    "         \"What is making it difficult for you to focus on your studies?\")\n",
    "    ]\n",
    "    \n",
    "    for pattern, response in patterns:\n",
    "        match = re.match(pattern, user_input, re.IGNORECASE)\n",
    "        if match and match.groups():\n",
    "            return response.format(reflect(match.group(1)))\n",
    "        elif match:\n",
    "            return response\n",
    "    return \"Tell me more about that.\"\n",
    "\n",
    "previous_questions = []\n",
    "sarcasm = [\n",
    "    \"We've been through this already. Try something new.\",\n",
    "    \"Seriously? Again? I thought we moved on.\",\n",
    "    \"Oh wow, you're stuck on that? Let's call it a day.\",\n",
    "    \"Asking me the same thing twice won't change anything.\",\n",
    "    \"What, did you forget I already answered that?\",\n",
    "    \"Congratulations, you've discovered the loop function.\",\n",
    "    \"I'm not a broken record, even if you're testing me.\"\n",
    "]\n",
    "\n",
    "print(\"ELIZA: Hello! How can I help you today?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \").strip()\n",
    "    \n",
    "    if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"ELIZA: Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    if not user_input:\n",
    "        continue\n",
    "    \n",
    "    if user_input.lower() in [q.lower() for q in previous_questions]:\n",
    "        print(f\"ELIZA: {random.choice(sarcasm)}\\n\")\n",
    "    else:\n",
    "        response = eliza_response(user_input)\n",
    "        print(f\"ELIZA: {response}\\n\")\n",
    "        previous_questions.append(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc02f8",
   "metadata": {},
   "source": [
    "## Exercise 2: Implementing RegEx on NLP\n",
    "Applies regex patterns to extract and process text data.\n",
    "\n",
    "### Part A: Extract Capitalized Words\n",
    "Use regex to find all words that start with a capital letter in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A: Extract words starting with uppercase\n",
    "import re\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"PART A: Extract Capitalized Words\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "text = \"\"\"Alice was beginning to get very tired of sitting by her sister on the bank,\n",
    "and of having nothing to do. Once or twice she had peeped into the book\n",
    "her sister was reading, but it had no pictures or conversations in it, \"and\n",
    "what is the use of a book,\" thought Alice, \"without pictures or conversations?\"\"\"\n",
    "\n",
    "pattern = r'\\b[A-Z]\\w*'\n",
    "words = re.findall(pattern, text)\n",
    "\n",
    "print(f\"Pattern: {pattern}\")\n",
    "print(f\"Capitalized words: {words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a00af",
   "metadata": {},
   "source": [
    "### Part B: Extract and Replace from Literary Text\n",
    "Read Moby Dick text file and replace the first 10 instances of \"Whale/whale\" with \"leviathan\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e622904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part B: Extract and replace Whale/whales in Moby Dick\n",
    "print(\"=\" * 50)\n",
    "print(\"PART B: Extract and Replace Whale/Whales\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    with open(r'c:\\Users\\DELL\\Desktop\\CCS-249_25-26_Activities\\BSCS 3A\\KYLA_ELIJAH_RAMIRO\\melville-moby_dick.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    pattern = r'\\b(Whale|Whales|whale|whales)\\b'\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    print(f\"Pattern: {pattern}\")\n",
    "    print(f\"Total matches: {len(matches)}\")\n",
    "    print(f\"First 10: {matches[:10]}\\n\")\n",
    "    \n",
    "    # Replace first 10 instances\n",
    "    counter = [0]\n",
    "    def replace_first_ten(m):\n",
    "        counter[0] += 1\n",
    "        return \"leviathan\" if counter[0] <= 10 else m.group(0)\n",
    "    \n",
    "    modified = re.sub(pattern, replace_first_ten, text)\n",
    "    print(f\"First 10 instances replaced with 'leviathan'\\n\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: melville-moby_dick.txt not found\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f4367",
   "metadata": {},
   "source": [
    "### Part C: Extract Character Dialogue from NLTK Corpus\n",
    "Use NLTK to load the pirates.txt file and extract all lines spoken by Jack Sparrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83906c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C: Extract Jack Sparrow lines from NLTK Pirates corpus\n",
    "print(\"=\" * 50)\n",
    "print(\"PART C: Extract Jack Sparrow Dialogue\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import webtext\n",
    "    \n",
    "    nltk.download('webtext', quiet=True)\n",
    "    \n",
    "    text = webtext.raw('pirates.txt')\n",
    "    pattern = r'JACK SPARROW:\\s*(.+?)(?=\\n[A-Z\\s]+:|$)'\n",
    "    \n",
    "    lines = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    print(f\"Pattern: {pattern}\")\n",
    "    print(f\"Total Jack Sparrow lines: {len(lines)}\\n\")\n",
    "    print(\"First 5 lines:\")\n",
    "    for i, line in enumerate(lines[:5], 1):\n",
    "        clean = line.strip().replace('\\n', ' ')[:80]\n",
    "        print(f\"{i}. {clean}...\\n\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"Error: NLTK not installed. Run: pip install nltk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naive_bayes_title",
   "metadata": {},
   "source": [
    "## Exercise 3: Na\u00efve Bayes Implementation\n",
    "Manual implementation of a Na\u00efve Bayes classifier for spam detection without using external packages/libraries.\n",
    "\n",
    "### Dataset:\n",
    "| Document | Class |\n",
    "|----------|-------|\n",
    "| Free money now!!! | SPAM |\n",
    "| Hi mom, how are you? | HAM |\n",
    "| Lowest price for your meds | SPAM |\n",
    "| Are we still on for dinner? | HAM |\n",
    "| Win a free iPhone today | SPAM |\n",
    "| Let's catch up tomorrow at the office | HAM |\n",
    "| Meeting at 3 PM tomorrow | HAM |\n",
    "| Get 50% off, limited time! | SPAM |\n",
    "| Team meeting in the office | HAM |\n",
    "| Click here for prizes! | SPAM |\n",
    "| Can you send the report? | HAM |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_bayes_dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: List of documents with their corresponding class labels\n",
    "documents = [\n",
    "    (\"Free money now!!!\", \"SPAM\"),\n",
    "    (\"Hi mom, how are you?\", \"HAM\"),\n",
    "    (\"Lowest price for your meds\", \"SPAM\"),\n",
    "    (\"Are we still on for dinner?\", \"HAM\"),\n",
    "    (\"Win a free iPhone today\", \"SPAM\"),\n",
    "    (\"Let's catch up tomorrow at the office\", \"HAM\"),\n",
    "    (\"Meeting at 3 PM tomorrow\", \"HAM\"),\n",
    "    (\"Get 50% off, limited time!\", \"SPAM\"),\n",
    "    (\"Team meeting in the office\", \"HAM\"),\n",
    "    (\"Click here for prizes!\", \"SPAM\"),\n",
    "    (\"Can you send the report?\", \"HAM\")\n",
    "]\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "print(f\"\\nFirst 3 documents:\")\n",
    "for i, (doc, label) in enumerate(documents[:3], 1):\n",
    "    print(f\"{i}. '{doc}' - {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bag_of_words_title",
   "metadata": {},
   "source": [
    "### Part A: Generate Bag of Words (Word Frequency)\n",
    "Creates a bag of words representation to count word frequencies across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bag_of_words_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text into lowercase words, removing punctuation.\"\"\"\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    \n",
    "    for char in text.lower():\n",
    "        if char.isalnum():\n",
    "            current_word += char\n",
    "        else:\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "                current_word = \"\"\n",
    "    \n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def create_bag_of_words(documents):\n",
    "    \"\"\"Generate bag of words with word frequencies.\"\"\"\n",
    "    bag_of_words = {}\n",
    "    \n",
    "    for doc, label in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        for token in tokens:\n",
    "            if token in bag_of_words:\n",
    "                bag_of_words[token] += 1\n",
    "            else:\n",
    "                bag_of_words[token] = 1\n",
    "    \n",
    "    return bag_of_words\n",
    "\n",
    "# Generate bag of words\n",
    "bag_of_words = create_bag_of_words(documents)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PART A: Bag of Words (Word Frequency)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal unique words in vocabulary: {len(bag_of_words)}\")\n",
    "print(f\"\\nWord frequencies (sorted by frequency):\")\n",
    "\n",
    "# Sort by frequency (descending)\n",
    "sorted_words = sorted(bag_of_words.items(), key=lambda x: x[1], reverse=True)\n",
    "for word, freq in sorted_words[:15]:\n",
    "    print(f\"  '{word}': {freq}\")\n",
    "\n",
    "print(f\"\\n... and {len(sorted_words) - 15} more words\" if len(sorted_words) > 15 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prior_probability_title",
   "metadata": {},
   "source": [
    "### Part B: Calculate Prior Probabilities\n",
    "Calculates the prior probability for each class (HAM and SPAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prior_probability_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prior(documents):\n",
    "    \"\"\"Calculate prior probabilities for each class.\"\"\"\n",
    "    class_counts = {}\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    for doc, label in documents:\n",
    "        if label in class_counts:\n",
    "            class_counts[label] += 1\n",
    "        else:\n",
    "            class_counts[label] = 1\n",
    "    \n",
    "    prior = {}\n",
    "    for label, count in class_counts.items():\n",
    "        prior[label] = count / total_docs\n",
    "    \n",
    "    return prior, class_counts\n",
    "\n",
    "# Calculate priors\n",
    "prior, class_counts = calculate_prior(documents)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PART B: Prior Probabilities\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal documents: {len(documents)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"  {label}: {count} documents\")\n",
    "\n",
    "print(f\"\\nPrior probabilities:\")\n",
    "for label, prob in prior.items():\n",
    "    print(f\"  P({label}) = {count}/{len(documents)} = {prob:.4f} ({prob*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likelihood_title",
   "metadata": {},
   "source": [
    "### Part C: Calculate Likelihood of Tokens\n",
    "Calculates the likelihood of each token in the vocabulary with respect to each class using Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likelihood_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(documents, vocabulary):\n",
    "    \"\"\"Calculate likelihood of each token given each class with Laplace smoothing.\"\"\"\n",
    "    # Count word occurrences per class\n",
    "    word_count_by_class = {}\n",
    "    total_words_by_class = {}\n",
    "    \n",
    "    # Initialize\n",
    "    for doc, label in documents:\n",
    "        if label not in word_count_by_class:\n",
    "            word_count_by_class[label] = {}\n",
    "            total_words_by_class[label] = 0\n",
    "    \n",
    "    # Count words\n",
    "    for doc, label in documents:\n",
    "        tokens = tokenize(doc)\n",
    "        for token in tokens:\n",
    "            if token in word_count_by_class[label]:\n",
    "                word_count_by_class[label][token] += 1\n",
    "            else:\n",
    "                word_count_by_class[label][token] = 1\n",
    "            total_words_by_class[label] += 1\n",
    "    \n",
    "    # Calculate likelihood with Laplace smoothing (alpha = 1)\n",
    "    likelihood = {}\n",
    "    vocab_size = len(vocabulary)\n",
    "    \n",
    "    for label in word_count_by_class:\n",
    "        likelihood[label] = {}\n",
    "        for word in vocabulary:\n",
    "            word_count = word_count_by_class[label].get(word, 0)\n",
    "            # Laplace smoothing: (count + 1) / (total_words + vocab_size)\n",
    "            likelihood[label][word] = (word_count + 1) / (total_words_by_class[label] + vocab_size)\n",
    "    \n",
    "    return likelihood, word_count_by_class, total_words_by_class\n",
    "\n",
    "# Calculate likelihood\n",
    "likelihood, word_count_by_class, total_words_by_class = calculate_likelihood(documents, bag_of_words.keys())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PART C: Likelihood of Tokens\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nVocabulary size: {len(bag_of_words)}\")\n",
    "print(f\"\\nTotal words per class:\")\n",
    "for label, count in total_words_by_class.items():\n",
    "    print(f\"  {label}: {count} words\")\n",
    "\n",
    "print(f\"\\nLikelihood with Laplace smoothing (alpha=1):\")\n",
    "print(f\"Formula: P(word|class) = (count(word in class) + 1) / (total words in class + vocab_size)\")\n",
    "\n",
    "# Show likelihood for some example words\n",
    "example_words = ['free', 'money', 'meeting', 'office', 'spam', 'ham']\n",
    "print(f\"\\nExample likelihood values:\")\n",
    "for word in example_words:\n",
    "    if word in bag_of_words:\n",
    "        print(f\"\\n  Word: '{word}'\")\n",
    "        for label in likelihood:\n",
    "            count = word_count_by_class[label].get(word, 0)\n",
    "            prob = likelihood[label][word]\n",
    "            print(f\"    P('{word}'|{label}) = ({count} + 1) / ({total_words_by_class[label]} + {len(bag_of_words)}) = {prob:.6f}\")\n",
    "\n",
    "# Show top words for each class\n",
    "print(f\"\\nTop 5 most frequent words per class:\")\n",
    "for label in word_count_by_class:\n",
    "    print(f\"\\n  {label}:\")\n",
    "    sorted_words = sorted(word_count_by_class[label].items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, count in sorted_words[:5]:\n",
    "        prob = likelihood[label][word]\n",
    "        print(f\"    '{word}': count={count}, P('{word}'|{label})={prob:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_title",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This implementation demonstrates a manual Na\u00efve Bayes classifier with:\n",
    "1. **Bag of Words**: Tracks word frequencies across all documents\n",
    "2. **Prior Probabilities**: P(HAM) and P(SPAM) based on document counts\n",
    "3. **Likelihood**: P(word|class) for each word in vocabulary using Laplace smoothing\n",
    "\n",
    "No external libraries were used - all calculations are done manually using basic Python data structures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}